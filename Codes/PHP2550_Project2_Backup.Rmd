---
title: "PHP2550_Project2_Backup"
author: "Jialin Liu"
date: "2023-11-13"
output: pdf_document
---

```{r, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
# Load libraries
library(tidyverse)
library(dplyr)
library(glmnet)
library(mice)
library(caret)
library(MASS)
library(data.table)
library(gt)
library(gtsummary)
library(flextable)
library(kableExtra)
library(ggplot2)

knitr::opts_chunk$set(echo =  F)
```

# Abstract
   **Background-** Existing prediction models based on large databases can estimate the likelihood of tracheostomy placement or death given baseline demographics and clinical diagnoses. However, these analyses have not used detailed respiratory parameters and have not provided prediction at different post-menstrual age (PMA). The predictive model is proposed for this purpose in this study. 
   
   **Methods-** We developed and internally validated logistic regression model in predicting the need for tracheostomy in infants with severe bronchopulmonary dysplasia. Variable selection methods were used to find the best subset of variables included in the logistic model, which include the lasso and forward stepwise selection methods. Both models were internally validated, and the discrimination and calibration were estimated. 
   
   **Results-** Notably, the forward stepwise regression model stands out as the top performer, with the highest AUC (91.6%) and the highest sensitivity (61.3%) among the three variable selection methods. While lasso models also demonstrate good performance, the forward stepwise model exhibits a slight edge in predictive accuracy. 
   
   **Conclusion-** The forward stepwise selection had better predictive accuracy than the lasso model in assessment of potential adverse outcome (tracheostomy placement or death) in infants who are born prematurely with chronic lung disease. 

# Introduction

Bronchopulmonary dysplasia (BPD), also known as chronic lung disease, causes long-term breathing problems in newborn babies especially for those who are born prematurely. As the most common complication of prematurity, this disease affects estimated 10,000-15,000 infants each year in the United States, which is caused many multifactorial individual characteristics both from genetic and epigenetic aspects and substantial impact infant's susceptibility. Compared to the healthy lung tissue which can support normal breathing, the lungs with BPD have fewer and larger the tiny air sacs of the lung (alveoli), causing tissue destruction (fibrosis and metaplasia) within the lungs and usually showing signs of respiratory distress, such as breathing quickly and grunting. This deficit in pulmonary vascular development has no cure, but it can be treated and most babies go on to live a long and healthy life. There are four levels of severity of BPD, in particular, 75% of babies with grade 3 BPD are always dependent on a ventilator at 36 weeks gestational age when they are discharged from the hospital. To allow babies to be hooked up to a ventilator for a long time, they needs a tracheostomy that is a surgical hole in the neck and tube inserted in the trachea to allow them in breath and out breath to lungs. Since some studies show that tracheostomy associated with improved outcomes within 4 months of age and a list of benefits to performing a tracheostomy, up to 12% babies with severe grade 3 BPD are required to have a tracheostomy. However, risks associated with a tracheostomy are also existing, which include increased risk of death compared to no tracheostomy, accidentally cannula obstruction or abscission, and increased rates of infection on skin, trachea and lungs.

Existing prediction models based on large databases can accurately estimate the likelihood of tracheostomy placement or death given baseline demographics and clinical diagnoses. However, these analyses have not used detailed respiratory parameters and have not provided prediction at different post-menstrual age (PMA). Accurate prediction the need for tracheostomy at early PMA would have implications for counseling of families and appropriate timing og tracheostomy placement, which is an active area of debate in severe BPD (sBPD). Motivated by deficiency in the previous work, models are designed to determine who really needs a tracheostomy, and when is an ideal time frame to refer a patient for tracheostomy. We will be using clinical data collected from multicenter, retrospective case-control study and recorded infants who are born at $\leq 32$ weeks and their respiratory support at 36 and 44 weeks PMA. Outcomes of interest (tracheostomy or death) are recorded at the time point when they were discharged from hospitals. We developed and validated two prediction models and compared the performance of their predictability with respect to eventual needs for tracheostomy or death prior to discharge. 

```{r}
trach_df <- read.csv("project2.csv")
```

# Methods
## Study Population
This study analyzed data from a national data set of demographic, diagnostic, and respiratory parameters of infants with sBPD admitted to collaborative Neonatal intensive care units (NICUs) across multiple centers. The data consists of 999 participants who are born at $\leq 32$ weeks and their corresponding 30 factors and outcomes of eventually healthy status at(or before) discharge. The rest of this section is devoted to describing summary statistics, procedures for preprocessing data, exploring any potential relationships between variables as well as missing values before building models with variable selection. We will conduct brief exploratory data analysis in terms of three aspects: birth and demographic variables, respiratory support variables, weight and tracheostomy placement at 36 and 44 weeks. 

Of those 999 patients' records, some duplicated patient ID with information have been detected and removed for further analysis. To make sure completeness of outcome variables, we found two places of missingness in `Death` outcome variable. One of them should be corrected as "No" death since this patient was discharged from hospital at 43 weeks without tracheostomy placement, thus it is reasonable to replace this missing value with known outcome based on our basic speculation. The another missing value in `Death` cannot be deduced as missing value appeared in hospital discharge gestational age `hosp_dc_ga`, without this supporting information, we couldn't make assumption upon these bunch lack of data so that we removed this particular patient from our observational data. For conciseness and fewer number of models needed to construct, we combined two outcomes of interest, `Trach` and `Death`, into one final outcome about healthy status that refers to "Yes(1)" when babies neither had tracheostomy placement nor died at(or before) discharge, and conversely, "No(0)" represents adverse outcome of health, equally saying, babies either had tracheostomy or died.

For hospital discharge `hosp_dc_ga` variable, there exists some values that lie far much away from the main body of observations and may distort summaries of the distribution. For example, some cases showed hospital discharge gestational ages are greater than 300 weeks, which seems to be irrational in this study, since, based on boxplot and interquartile range, most often patients were discharged around 40-50 weeks. Therefore, we planned to simply remove those few cases. Additionally, the data has been collected from multiple centers, we wanted to check if number of observations are evenly distributed and balanced. We found that center 20 and 21 only consisted with a total of 5 patients, which sample size are too small to conduct further analysis on these two centers. So we decided to remove those 5 participants in center 21 and 20 from the whole dataset. In addition, levels in maternal race variable did not align with specified categories shown in the code book. Without explanation for this error, we removed this variable. For model simplicity, we considered respiratory and diagnostic related variables measured at 36 weeks only, and further analyses will be considered to add later time points and give a more comprehensive insight into an ideal time point to refer patients for tracheostomy. 

```{r}
##== remove duplicated id
duplicate_id <- unique(trach_df$record_id)[table(trach_df$record_id) >1]
trach_df <- trach_df[-which(trach_df$record_id==duplicate_id)[-1],]

##== remove death NA and transfer one NA to no
which.death.na <- which(is.na(trach_df$Death))
trach_df[which.death.na[1],]$Death <- "No"
trach_df <- trach_df[-which.death.na[2],]

##== remove hospital discharge gestational age
trach_df <- trach_df[-which(trach_df$hosp_dc_ga >= 250),]
# boxplot(trach_df$hosp_dc_ga)

##== factorize all categorical variables
chr_names <- c('mat_ethn','del_method', 'prenat_ster' ,'com_prenat_ster', 'mat_chorio', 'ventilation_support_level.36', 'gender', 'sga', 'any_surf', 'med_ph.36', 'Death')
trach_df[,chr_names] <- lapply(trach_df[,chr_names] , factor)
```

```{r}
##== Combine outcomes to one variable
trach_df$adverse_outcome <- ifelse(trach_df$Trach==0 & trach_df$Death == "No", 0, 1)
trach_df$adverse_outcome <- as.factor(trach_df$adverse_outcome)

##== remove center 21 and 20, and trach & death columns
trach_df <- trach_df %>% filter(!center %in% c(20,21)) %>% select(-c("record_id", "mat_race", "Trach", "Death"))

##== Subset 36 weeks data
trach_36_df <- trach_df %>% select(-c("weight_today.44", "ventilation_support_level_modified.44", "inspired_oxygen.44", "p_delta.44", "peep_cm_h2o_modified.44", "med_ph.44"))
##== factorize center variable
trach_36_df$center <- as.factor(trach_36_df$center)
```

Table 1 describes summary statistics for a part of participant birth and demographic variables, stratified by outcome of interest `adverse_outcome`, with the sample size for healthy group being 806 (noted as N = 806) and for group who either had tracheostomy placement or died prior to discharge (N = 182). Birth variables, which include birth weight (in g) `bw`, birth length (in centimeters) `blength`, and head circumference at birth `birth_hc`, show statistically significant differences between healthy and non-healthy groups since all p-values are greatly less than significance level ($\alpha = 0.05$). In particular, we could observe that those babies who had adverse outcome are high likely to have lower birth weights (mean of 757g) and smaller hear circumference (mean of 22.89cm), probably due to prematurity, than those who hadn't tracheostomy placement or died at discharge birth weights (mean of 816g) and larger head circumference (mean of 23.25cm). Delivery Method `del_method` was reported as categorical data with two methods: 1 represents for vaginal delivery and 2 represents for cesarean section. Higher percentage of babies who were delivered by cesarean section experienced adverse outcome than those who were delivered by vaginal method, with a significance difference between two groups based on a small p-value from Chi-squared test. One notable thing is that `any_surf` to record if the infant receive surfactant in the first 72 hours consisted with 44% of missingness in the original data set, thus it is crucial to carefully criticize whether the assumptions of multiple imputation are likely to hold and this variable cannot be reasonably imputed from the other available data by multiple imputation method. 

```{r, warning=FALSE}
##== summary table by outcome 
Table1 <- trach_36_df %>% 
  dplyr::select(c('bw','blength', 'birth_hc', 'del_method', 'prenat_ster', 'sga', 'any_surf', 'adverse_outcome')) %>%
  tbl_summary(
    missing = "no",
    by = adverse_outcome,
    type = list(where(is.numeric) ~ 'continuous2',
                prenat_ster ~ 'categorical',
                any_surf ~ 'categorical'),
    statistic = list(
    all_continuous() ~ c("{mean} ({max}, {sd})"),
    all_categorical() ~ "{n} ({p}%)"
    )
  ) %>%
  bold_labels() %>%
  add_n(statistic = "{n_miss} ({p_miss}%)") %>%
  add_p() %>%
  modify_header(n = "**Missing**") %>%
  as_flex_table() %>%
  flextable::set_caption(caption="Participants Baseline Demographics Variables")
Table1
# save_as_image(Table1, path = "/Users/jialinliu/Desktop/PHP 2550/Project2/Table1.png", zoom = 3, expand = 15, webshot = "webshot")
```

Table 2 provides data on respiratory-related variables measured at 36 weeks across different centers with an additional column for missing center name. For weight at 36 weeks `weight_today.36` variable, statistically significant p-value (0.021) is measured to assess a tendency of weights in at least one of the groups to be different than weights in at least one of the other group. The mean weight ranges from 2,073 grams in center 1 to 1,922 grams in center 5, with varying levels of missing data, potentially indicating variability in the mean weights at 36 weeks across multiple centers. For categorical ventilation support level at 36 weeks `ventilation_support_level.36` with three levels (0 means no respiratory support; 1 means non-invasive positive pressure; and 2 means invasive positive pressure) shows a significant difference across different centers (p < 0.001) by the Pearson's Chi-squared test. Even though Chi-squared test is the most commonly used test for assessing difference in distribution of a categorical variable between two or more independent groups (`center` here), the Chi-squared approximation to the distribution of the test statistic relies on the counts being roughly normally distributed. Because many of cell sizes are very small being less than 5 observations, the approximation may be poor. Instead, we run the Chi-squared test with computation of the simulated p-values, and resulted in a strong association between `center` and `ventilation_support_level.36` variables. The mean fraction of inspired oxygen at 36 weeks `inspired_oxygen.36` also varies significantly (p < 0.001) across centers. Some centers reported higher proportion of missingness, such as 43% in center 12. The peak inspiratory pressure ($cmH_2O$) at 36 weeks shows significant difference in at least one center (p < 0.001) compared to others, and a still high percentage of missingness observed in center 12, as well as in positive and exploratory pressure ($cmH_2O$) at 36 weeks `peep_cm_h2o.36` variable. For medication for pulmonary hypertension at 36 weeks `med_ph.36`, we could reject the null hypothesis and conclude that there is a strong association between center and medication for pulmonary hypertension given a significantly small p-value. Lastly, the mean hospital discharge gestational age `hosp_dc_ga` appears to have significant differences across centers, with a 100% missing rate in center 4 and 98% of missingness in center 1. There is notable variability of missingness in these respiratory variables at 36 weeks across centers, for example, center 12 consisted with almost half missing values in variables related to inspiratory and exploratory pressure and inspired oxygen, and some centers did not record hospital discharge gestational age at all. These missing data rates are concerning, which could impact the reliability of the statistical analyses.

When considering interaction terms for a logistic regression model, we consider interaction between center and respiratory support variables since there might be center-specific variation or patient population differences that associates with the level of respiratory support provided. 

```{r}
##== define custom test
chisq.test.simulate.p.values <- function(data, variable, by, ...) {
  result <- list()
  test_results <- stats::fisher.test(data[[variable]], data[[by]], simulate.p.value = TRUE)
  result$p <- test_results$p.value
  result$test <- test_results$method
  result
}
##== summary table by center
trach_36_df %>% 
  mutate(center = fct_explicit_na(center)) %>%
  dplyr::select(-c('mat_ethn', 'bw','ga', 'blength', 'birth_hc', 'del_method', 'prenat_ster', 'com_prenat_ster', 'mat_chorio', 'gender', 'sga', 'any_surf', 'adverse_outcome')) %>%
  tbl_summary(
    missing = "no",
    by = center,
    type = list(where(is.numeric) ~ 'continuous2'),
    statistic = list(
    all_continuous() ~ c("{mean} ({sd})",
                               "{N_miss} ({p_miss}%)"),
    all_categorical() ~ "{n} ({p}%)"
    )
  ) %>%
  bold_labels() %>%
  add_p(all_categorical() ~ "chisq.test.simulate.p.values") %>%
  modify_footnote(everything() ~ NA) %>%
  as_kable_extra(booktabs = TRUE, caption = "Summary Statistics of Respiratory Variables at 36 Weeks by Center") %>%
  kableExtra::footnote()%>%
  kableExtra::kable_styling(font_size = 18, latex_options="scale_down") 
```


## Multiple Imputation

Multiple imputation is applied to address missing values by creating 5 complete datasets. This method is trying to handle with each missing entry by estimating multiple reliable values such as regression models, running analysis across those completed dataset, such as mean values over each imputed datasets, and finally aggregating all previous analyses results, such as taking average over those mean values, and analyzing how far they spread
out in terms of standard deviations and confidence intervals to identify deviance between imputed missing values. We constructed train-test splitted data and fitted the imputation model on the train data and apply the model to impute the test data. Given 5 complete train datasets, we will run lasso and forward stepwise regression for variable selection and use combined 5 test data as a validation dataset to assess performance of each model. 

```{r, warning=FALSE}
set.seed(1)
ignore <- sample(c(TRUE, FALSE), size = nrow(trach_36_df), replace = TRUE, prob = c(0.25, 0.75))

trach_36_df_mice_out <- mice(trach_36_df, m = 5, ignore = ignore, print = FALSE, seed = 2550)
imp.train <- filter(trach_36_df_mice_out, !ignore)
# imp.train$data

# Store each imputed data set
trach_36_df_imp_train <- vector("list",5)    
for (i in 1:5){
  trach_36_df_imp_train[[i]] <- mice::complete(imp.train,i) 
}
# trach_36_df_imp_train[[1]]

# Store imputated the test data 
imp.test <- filter(trach_36_df_mice_out, ignore)
trach_36_df_imp_test <- vector("list",5)    
for (i in 1:5){
  trach_36_df_imp_test[[i]] <- mice::complete(imp.test,i) 
}
# trach_36_df_imp_test[[2]]
trach_36_df_test <- rbindlist(trach_36_df_imp_test)
```

## Selection of Variables for the Model

```{r, warning=FALSE}
##== Lasso
  
lasso <- function(df) { 
  #' Runs 10-fold CV for lasso and returns corresponding coefficients 
  #' @param df, data set
  #' @return coef, coefficients for minimum cv error
  
  # Matrix form for ordered variables  
  x.ord <- model.matrix(adverse_outcome ~. + center:ventilation_support_level.36 + center:inspired_oxygen.36 + center:p_delta.36 + center:peep_cm_h2o_modified.36 + center:med_ph.36, data = df)[,-1] #this model.matrix is the place to add interaction and transformation, and run summary of model after cross validation; if transformation and interaction not work well, review x.ord and change form of covariates, and rerun cross validation

  y.ord <- df$adverse_outcome
  
  # Generate flods
  k <- 10 
  set.seed(1)
  folds <- sample(1:k, nrow(df), replace=TRUE)
  
  # Lasso model
  lasso_mod_cv <- cv.glmnet(x.ord, y.ord, nfolds = 10, foldid = folds, 
                         alpha = 1, family = "binomial") 
  
  lasso_mod <- glmnet(x.ord, y.ord, nfolds=10, foldid=folds,
                      alpha=1, family = "binomial",
                      lambda = lasso_mod_cv$lambda.min)
  # Get coefficients 
  coef <- coef(lasso_mod)
  return(coef)
} 

# Find average lasso coefficients over imputed datasets
lasso_coef1 <- lasso(trach_36_df_imp_train[[1]]) 
lasso_coef2 <- lasso(trach_36_df_imp_train[[2]]) 
lasso_coef3 <- lasso(trach_36_df_imp_train[[3]]) 
lasso_coef4 <- lasso(trach_36_df_imp_train[[4]]) 
lasso_coef5 <- lasso(trach_36_df_imp_train[[5]]) 
lasso_coef <- cbind(lasso_coef1, lasso_coef2, lasso_coef3, 
                    lasso_coef4, lasso_coef5) 
lasso_coef_df <- data.frame(variables =  rownames(as.data.frame(as.matrix(lasso_coef1))),
                            coef1 = as.data.frame(as.matrix(lasso_coef1))[,1],
                            coef2 = as.data.frame(as.matrix(lasso_coef2))[,1],
                            coef3 = as.data.frame(as.matrix(lasso_coef3))[,1],
                            coef4 = as.data.frame(as.matrix(lasso_coef4))[,1],
                            coef5 = as.data.frame(as.matrix(lasso_coef5))[,1])
lasso_coef_df$lasso_coef_final_avg <- apply(lasso_coef_df[,-1], 1, mean) 

times_zero <- vector()
for (i in 1:nrow(lasso_coef_df)){
    times_zero_temp <- sum(lasso_coef_df[i,2:6] == 0)
    times_zero <- c(times_zero, times_zero_temp)
}
lasso_coef_df$times_zero <- times_zero
lasso_coef_df$lasso_coef_final2 <- lasso_coef_df$lasso_coef_final_avg
lasso_coef_df$lasso_coef_final2[lasso_coef_df$times_zero >= 3] <- 0

lasso_coef_df2 <- lasso_coef_df
colnames(lasso_coef_df2) <- c("Variables", "Coef1", "Coef2", "Coef3", "Coef4", "Coef5", "Lasso 1", "time of zeros", "Lasso 2")

lasso_coef_table <- data.frame(cbind(c(lasso_coef_df$variables), lasso_coef_df$lasso_coef_final_avg, lasso_coef_df$lasso_coef_final2))
rownames(lasso_coef_table) <- lasso_coef_df$variables
colnames(lasso_coef_table) <- c("Variable", "Lasso1", "Lasso2")
lasso_coef_table <- lasso_coef_table[,-c(1)]
lasso_coef_table <- lasso_coef_table[lasso_coef_table$Lasso1!=0,]
lasso_coef_table$Lasso1 <- as.numeric(lasso_coef_table$Lasso1)
lasso_coef_table$Lasso2 <- as.numeric(lasso_coef_table$Lasso2)
lasso_coef_table %>% round(3) %>% kbl(caption = "Coefficients of Lasso Regression Models",
                                      align = "c", booktabs = T) %>%   kable_styling(full_width=T,latex_options = c('HOLD_position'))
```

```{r, warning=FALSE, eval=FALSE}
##== Forward stepwise selection

# Set seed for reproducibility
set.seed(1)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
# forward_coef <- data.table()
# for (m in 1:5){
#   step.model <- train(adverse_outcome ~., data = trach_36_df_imp_train[[m]],
#                     method = "glmStepAIC", 
#                     family = "binomial",
#                     trControl = train.control, 
#                     trace = F)
#   forward_coef_temp <- coef(step.model$finalModel, 4)
#   forward_coef = cbind(forward_coef, forward_coef_temp)
# }
  step.model.1 <- train(adverse_outcome ~., data = trach_36_df_imp_train[[1]],
                    method = "glmStepAIC", 
                    family = "binomial",
                    trControl = train.control, 
                    trace = F)
  forward_coef.1 <- coef(step.model.1$finalModel, 4)
  
  step.model.2 <- train(adverse_outcome ~., data = trach_36_df_imp_train[[2]],
                    method = "glmStepAIC", 
                    family = "binomial",
                    trControl = train.control, 
                    trace = F)
  forward_coef.2 <- coef(step.model.2$finalModel, 4)
  
  step.model.3 <- train(adverse_outcome ~., data = trach_36_df_imp_train[[3]],
                    method = "glmStepAIC", 
                    family = "binomial",
                    trControl = train.control, 
                    trace = F)
  forward_coef.3 <- coef(step.model.3$finalModel, 4)
  
  step.model.4 <- train(adverse_outcome ~., data = trach_36_df_imp_train[[4]],
                    method = "glmStepAIC", 
                    family = "binomial",
                    trControl = train.control, 
                    trace = F)
  forward_coef.4 <- coef(step.model.4$finalModel, 4)
  
  step.model.5 <- train(adverse_outcome ~., data = trach_36_df_imp_train[[5]],
                    method = "glmStepAIC", 
                    family = "binomial",
                    trControl = train.control, 
                    trace = F)
  forward_coef.5 <- coef(step.model.5$finalModel, 4)
  
  # forward_coef = cbind(forward_coef, forward_coef_temp)

# colnames(forward_coef) <- c("forward_coef1", 
#                             "forward_coef2", 
#                             "forward_coef3", 
#                             "forward_coef4", 
#                             "forward_coef5")

# form the table
forward_coef_table <- data.frame(cbind(c(lasso_coef_df$virable_names[1:28]), lasso_coef_df$lasso_coef_final_avg[1:28]))
rownames(forward_coef_table) <- lasso_coef_df$virable_names[1:28]
colnames(forward_coef_table) <- c("Variable", "Lasso")
forward_coef_table$Forward.1 <- 0 
forward_coef_table$Forward.2 <- 0 
forward_coef_table$Forward.3 <- 0 
forward_coef_table$Forward.4 <- 0 
forward_coef_table$Forward.5 <- 0 
forward_coef_table[names(forward_coef.1), "Forward.1"] <- forward_coef.1
forward_coef_table[names(forward_coef.2), "Forward.2"] <- forward_coef.2
forward_coef_table[names(forward_coef.3), "Forward.3"] <- forward_coef.3
forward_coef_table[names(forward_coef.4), "Forward.4"] <- forward_coef.4
forward_coef_table[names(forward_coef.5), "Forward.5"] <- forward_coef.5
forward_coef_table <- forward_coef_table[, -c(1,2)]
forward_coef_table$forward_coef_final_avg <-  apply(forward_coef_table, 1, mean) 
```



```{r}
forward_coef_table <- readRDS("forward_coef_table.RDS")
forward_coef_table$Avg_coef <- forward_coef_table$forward_coef_final_avg
forward_coef_table[,-6] %>% round(3) %>% 
  kbl(
        caption = "Coefficients of Forward Stepwise Selection 
        Models",
        align = "c",
        booktabs = T) %>% 
  kable_styling(full_width=F,latex_options = c('HOLD_position')) %>%
  row_spec(c(16, 22, 23, 24, 26), font_size=9)
```

After multiple imputation method, we have 5 imputed datasets for which we do cross-validation in each imputed training dataset. Cross-validation helps with overfitting issues and with generalizability of the lasso model. Then since we will have for each fold in cross-validation results for different lambdas, we choose the lambda that has the lowest model error. This means that we will have five sets of estimated coefficients (one for each imputed data set). We considered two possible solutions to generate final lasso model: The first method **Lasso 1** is simply averaging over all 5 sets of coefficients to obtain the final lasso model. The second method **Lasso 2** is counting the number of times each variable being selected, if more than or equal to 3 times the variable was not selected, we will force to remove the variable from the final lasso model. In other words, if some variables occasionally were not selected, we wanted to neglect occasional situations and averaging over all estimated coefficients from 5 imputation datasets. In order to include transformations or interactions in the model, it is needed to have an idea of which interactions or transformations need to be included based on explanatory analysis or prior professional knowledge before applying the cross-validation to the imputed datasets. So first, after we have the imputed data, we specified and placed interaction terms between center and respiratory parameters for predictors of interest, then, we fit the lasso model with the interactions that are of interest and evaluate their significance by running summary of model after cross-validation. Table 3 provides us with a list of final estimated coefficients with interaction terms under lasso models of 10-fold cross-validation on the train imputation datasets. Interaction terms between center and respiratory variables show strong association which impact the outcome of adverse outcome, such as center with medication for pulmonary hypertension at 35 weeks, and center with ventilation support level at 36 weeks. Thus, we could identify center-specific variation that associates with the level of respiratory support provided. In addition, main effects of `center` also perform great differences across centers on the outcome of babies healthy status. 


Except for lasso regression models, to preserve generalizability and overfitting, we planned to remove all interaction terms between center and other respiratory variables, instead, we only included main effects of covariates and perform forward stepwise selection with cross-validation for each imputed data set. Starting with the empty model and sequentially adding predictors to the model, one at a time, and choosing the best predictor at each step based on a criterion like AIC and BIC, but it can be seen as a "locally optimal", instead of globally optimal in the sense of the best subset. Table 4 provides coefficients for each imputed data set and averaging over 5 coefficients together to obtain the final forward stepwise model. To compare the estimated coefficients using different variable selection or shrinkage methods we found that intercept values vary a lot across different methods, with lasso method having the most negative value (-6.067), which means the log odds of adverse outcome while all other variables are held at zero. For variables with non-zero coefficients, a positive coefficient indicates a positive association between the variable and the odds of outcome. In forward selection model, `pre_nat_sterYes` has a positive average coefficient (1.282), indicating that having a prenatal corticosteroids are 1.282 times the odds of adverse outcome compared to those who did not have prenatal corticosteroids. Conversely, center 4 (-7.182) is negatively associated with the odds of adverse outcome, in other words, center 4 is least likely to have adverse outcome compared to other centers adjusting for remaining variables. For those variables with zero coefficients all 5 times represents the least importance to be include in predicting adverse outcome of patients.


# Internal Validation
```{r}
test.x.ord <- model.matrix(adverse_outcome ~. + center:ventilation_support_level.36 + center:inspired_oxygen.36 + center:p_delta.36 + center:peep_cm_h2o_modified.36 + center:med_ph.36, data = trach_36_df_test)[,-1]

test.forward.x.ord <- model.matrix(adverse_outcome ~., data = trach_36_df_test)[,-1]

test_y_lasso.1 <- lasso_coef_df$lasso_coef_final2[1] + test.x.ord %*% lasso_coef_df$lasso_coef_final2[-1]
test_y_lasso.1 <- exp( test_y_lasso.1 ) / ( 1 + exp( test_y_lasso.1 ) )
trach_36_df_test$predict_y_lasso.1 <- ifelse(test_y_lasso.1 < 0.5, 0, 1)

test_y_lasso.2  <- lasso_coef_df$lasso_coef_final_avg[1] + test.x.ord %*% lasso_coef_df$lasso_coef_final_avg[-1]
test_y_lasso.2 <- exp( test_y_lasso.2 ) / ( 1 + exp( test_y_lasso.2 ) )

trach_36_df_test$predict_y_lasso.2 <- ifelse(test_y_lasso.2 < 0.5, 0, 1)

test_y_forward <- forward_coef_table$forward_coef_final_avg[1] + test.forward.x.ord %*% forward_coef_table$forward_coef_final_avg[-1]
test_y_forward <- exp( test_y_forward ) / ( 1 + exp( test_y_forward ) )
trach_36_df_test$predict_y_forward <- ifelse(test_y_forward < 0.5, 0, 1)

```

We validated three models by assessing their performance on internal validation dataset obtained from multiple imputation method. Then we plot ROC curve and use AUC score to evaluate model fitting. According to scores and graphical means, we can look at how well our model differentiates between the two classes. The plot of ROC curve depicts classification model with a bigger area under the ROC curve explains and predicts outcome of interest better. By comparison, the forward stepwise selection model on the test data (AUC = 91.6%) explains the outcome of adverse outcome is better than two lass methods with consideration of interaction terms (AUC = 91.2% for second lasso model, and AUC = 91.1% for first lasso model). All three models have achieved an AUC greater than 0.8, which is a clear sign of good model performance. The AUC values are in the range of 0.911 to 0.916, suggesting that these models can be considered highly effective in distinguishing between the positive and negative classes. 

```{r, message=FALSE, warning=FALSE}
library(pROC)
par(mfrow = c(1,3))
roc_mod_lasso.1 <- roc(predictor=test_y_lasso.1 , 
               response=as.factor(trach_36_df_test$adverse_outcome), 
               levels = c(0,1), direction = "<")
plot(roc_mod_lasso.1, print.auc=TRUE, print.thres = TRUE, main = "ROC for Lasso 1")

roc_mod_lasso.2 <- roc(predictor=test_y_lasso.2, 
               response=as.factor(trach_36_df_test$adverse_outcome), 
               levels = c(0,1), direction = "<")
plot(roc_mod_lasso.2, print.auc=TRUE, print.thres = TRUE, main = "ROC for Lasso 2")

roc_mod_forward <- roc(predictor=test_y_forward, 
               response=as.factor(trach_36_df_test$adverse_outcome), 
               levels = c(0,1), direction = "<")
plot(roc_mod_forward, print.auc=TRUE, print.thres = TRUE, main = "ROC for forward")
```

```{r}
lasso.1_outcome <- table(trach_36_df_test$adverse_outcome, trach_36_df_test$predict_y_lasso.1)
# Confusion matrix (test data)
rownames(lasso.1_outcome) <- paste("Actual", rownames(lasso.1_outcome), sep = ":")
colnames(lasso.1_outcome) <- paste("Pred", colnames(lasso.1_outcome), sep = ":")
kable(lasso.1_outcome,
      caption = "Confusion Matix of Test Data under Lasso 1 Estimation",
      booktabs=T, escape=F, align = "c")%>%
  kable_styling(full_width = FALSE, latex_options = c('hold_position'))

lasso.2_outcome <- table(trach_36_df_test$adverse_outcome, trach_36_df_test$predict_y_lasso.2)
# confusion matrix (test data)
rownames(lasso.2_outcome) <- paste("Actual", rownames(lasso.2_outcome), sep = ":")
colnames(lasso.2_outcome) <- paste("Pred", colnames(lasso.2_outcome), sep = ":")
kable(lasso.2_outcome,
      caption = "Confusion Matix of Test Data under Lasso 2 Estimation",
      booktabs=T, escape=F, align = "c")%>%
  kable_styling(full_width = FALSE, latex_options = c('hold_position'))

forward_outcome <- table(trach_36_df_test$adverse_outcome, trach_36_df_test$predict_y_forward)
# confusion matrix (test data)
rownames(forward_outcome) <- paste("Actual", rownames(forward_outcome), sep = ":")
colnames(forward_outcome) <- paste("Pred", colnames(forward_outcome), sep = ":")
kable(forward_outcome,
      caption = "Confusion Matix of Test Data under Forward Stepwise Estimation",
      booktabs=T, escape=F, align = "c")%>%
  kable_styling(full_width = FALSE, latex_options = c('hold_position'))
```

Each row in a confusion matrix represents an actual target, while each column represents a predicted target. Under the first lasso method (averaging over all coefficients) the first row of the matrix evaluated on the test data considers healthy patients (the False class): 989 were correctly classified as no adverse outcome (True negative), while the remaining 5 was wrongly classified as adverse outcome (False positive). The second row considers unhealthy status, 119 patients are correctly classified in the positive prediction class(True positive), while the False positive was 111. Compared among those confusion matrices under different variable selection methods, we realized that forward stepwise selection made the highest number of true positive prediction (141 versus 126 or 111) on the test data set. 

Sensitivity, true positive rate, indicates the percentage of individuals the model correctly predicted patients who had adverse outcome, which is the highest done by forward stepwise selection model. On the contrary, specificity, true negative rate, represents the percentage of individuals the model correctly predicted would not have adverse outcome. And the accuracy-test from the confusion matrix on the test dataset is calculated and is found to be 0.9073 in forward stepwise method, which slighlty outweighs the prediction performance made by lasso method (Accuracy are 0.8983 and 0.8943, respectively). 

```{r}
# sens <- tab_outcome[2,2]/(tab_outcome[2,1]+tab_outcome[2,2])
# spec <- tab_outcome[1,1]/(tab_outcome[1,1]+tab_outcome[1,2])
# ppv <- tab_outcome[2,2]/(tab_outcome[1,2]+tab_outcome[2,2])
# npv <- tab_outcome[1,1]/(tab_outcome[1,1]+tab_outcome[2,1])
# acc <- (tab_outcome[1,1]+tab_outcome[2,2])/sum(tab_outcome)

confu.test <- data.frame(
  Sensitivity = c(lasso.1_outcome[2,2]/(lasso.1_outcome[2,1]+lasso.1_outcome[2,2]),
                  lasso.2_outcome[2,2]/(lasso.2_outcome[2,1]+lasso.2_outcome[2,2]),                      forward_outcome[2,2]/(forward_outcome[2,1]+forward_outcome[2,2])),
  Specificity = c(lasso.1_outcome[1,1]/(lasso.1_outcome[1,1]+lasso.1_outcome[1,2]),
                  lasso.2_outcome[1,1]/(lasso.2_outcome[1,1]+lasso.2_outcome[1,2]),
                  forward_outcome[1,1]/(forward_outcome[1,1]+forward_outcome[1,2])),
  PositivePredicted = c(lasso.1_outcome[1,1]/(lasso.1_outcome[1,1]+lasso.1_outcome[1,2]),
                        lasso.2_outcome[1,1]/(lasso.2_outcome[1,1]+lasso.2_outcome[1,2]),
                        forward_outcome[1,1]/(forward_outcome[1,1]+forward_outcome[1,2])),
  NegativePredicted = c(lasso.1_outcome[1,1]/(lasso.1_outcome[1,1]+lasso.1_outcome[1,2]),
                        lasso.2_outcome[1,1]/(lasso.2_outcome[1,1]+lasso.2_outcome[1,2]),
                        forward_outcome[1,1]/(forward_outcome[1,1]+forward_outcome[1,2])),
  Accuracy = c((lasso.1_outcome[1,1]+lasso.1_outcome[2,2])/sum(lasso.1_outcome),
  (lasso.2_outcome[1,1]+lasso.2_outcome[2,2])/sum(lasso.2_outcome),
  (forward_outcome[1,1]+forward_outcome[2,2])/sum(forward_outcome))
) 

rownames(confu.test) <- c("Lasso 1", "Lasso 2", "Forward")
confu.test %>%
  kbl(
      caption = "Measurement of Confusion Matix of Test Data",
      booktabs=T, escape=F, align = "c")%>%
  kable_styling(full_width = FALSE, latex_options = c('hold_position'))
```

```{r}
num_cuts <- 10
calib_data_lasso1 <-  data.frame(prob = test_y_lasso.1,
                          bin = cut(test_y_lasso.1, breaks = num_cuts),
                          class = as.numeric(trach_36_df_test$adverse_outcome)-1)
calib_data_lasso1 <- calib_data_lasso1 %>% 
             group_by(bin) %>% 
             summarize(observed = sum(class)/n(), 
                       expected = sum(prob)/n(), 
                       se = sqrt(observed*(1-observed)/n()))
lasso1 <- ggplot(calib_data_lasso1) + 
  geom_point(aes(x = expected, y = observed)) +
  geom_errorbar(aes(x = expected, ymin=observed-1.96*se, 
                    ymax=observed+1.96*se), 
                colour="black", width=.01)+
  geom_abline(intercept = 0, slope = 1, color="red") +
  
  labs(x="Expected Proportion", y="Observed Proportion", title = "Calibratioin for Lasso Method 1") +
  theme_minimal()
```


```{r}
calib_data_lasso2 <-  data.frame(prob = test_y_lasso.2,
                          bin = cut(test_y_lasso.2, breaks = num_cuts),
                          class = as.numeric(trach_36_df_test$adverse_outcome)-1)
calib_data_lasso2 <- calib_data_lasso2 %>% 
             group_by(bin) %>% 
             summarize(observed = sum(class)/n(), 
                       expected = sum(prob)/n(), 
                       se = sqrt(observed*(1-observed)/n()))
lasso2 <- ggplot(calib_data_lasso2) + 
  geom_errorbar(aes(x = expected, ymin=observed-1.96*se, 
                    ymax=observed+1.96*se), 
                colour="black", width=.01)+
  geom_point(aes(x = expected, y = observed)) +
    geom_abline(intercept = 0, slope = 1, color="red") + 
  labs(x="Expected Proportion", y="Observed Proportion", title = "Calibratioin for Lasso Method 2") +
  theme_minimal()
```

```{r}
calib_data_forward <-  data.frame(prob = test_y_forward,
                          bin = cut(test_y_forward, breaks = num_cuts),
                          class = as.numeric(trach_36_df_test$adverse_outcome)-1)
calib_data_forward <- calib_data_forward %>% 
             group_by(bin) %>% 
             summarize(observed = sum(class)/n(), 
                       expected = sum(prob)/n(), 
                       se = sqrt(observed*(1-observed)/n()))
forward <- ggplot(calib_data_forward) + 
  geom_errorbar(aes(x = expected, ymin=observed-1.96*se, 
                    ymax=observed+1.96*se), 
                colour="black", width=.01)+
  geom_point(aes(x = expected, y = observed)) +
    geom_abline(intercept = 0, slope = 1, color="red") + 
  labs(x="Expected Proportion", y="Observed Proportion", title = "Calibratioin Plot for Forward Stepwise Selection") +
  theme_minimal()
```

```{r, message=FALSE}
library(gridExtra)
grid.arrange(lasso1, lasso2, forward, ncol = 2)
```

Another useful plot is a calibration plot. This type of plot groups the data by the estimated probabilities and compares the mean probability with the observed proportion of observations in class 1. It visualizes how close our estimated distribution and true distribution are to each other. The following three calibration plots all show partially close alignment with a 45-degree line, indicating good calibration. Forward stepwise selection model seems to have the closest alignment with the 45-degree line.

In summary, all three models perform well with respect to accuracy and well-calibration on the internal validation data set. The forward stepwise model shows slightly better performance in both discrimination and calibration compared to the lasso model. 

# Discussion 

At this time our findings have some limitations. First, multiple imputation method can calculate much more unbiased estimates compared to single imputation, however, this technique cannot perform well in case of missing not at random (MNAR) and limitations of sample size. Thus, our multiple imputation may not be highly reliable since possible violation of assumptions. Secondly, to avoid overfitting and generalizability, we should be aware of many potential interaction terms between center and other variables. The reason why lasso methods did not perform as top as forward stepwise models is highly related to overfitting caused by inclusion of many interaction terms in the lasso method. Other than that, forward stepwise method has its own limitation: it can be only seen as a "locally optimal", instead of globally optimal in the sense of the best subset. Thus, further improvement analysis should add best subset method and assess predictive accuracy among these three variable selection methods. 
